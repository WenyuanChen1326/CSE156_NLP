{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQxvO6dhMvtk"
   },
   "source": [
    "# CSE 156: Statistical NLP UCSD Assignment 2\n",
    "## Exploring Word Vectors (12.5 points + 2 bonus points)\n",
    "### <font color='blue'> Due 11:59pm, Monday April 18, 2022 </font>\n",
    "\n",
    "\n",
    "Before you start, make sure you read the README.txt in the same directory as this notebook.\n",
    "\n",
    "\n",
    "**Notes:** Please make sure to save the notebook as you go along. Submission Instructions are located at the bottom of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2041,
     "status": "ok",
     "timestamp": 1649549290401,
     "user": {
      "displayName": "Ndapandula Nakashole",
      "userId": "12501637879061685337"
     },
     "user_tz": 420
    },
    "id": "kCkBxLqTMvtn",
    "outputId": "bfe197d9-3b2f-4d56-d77d-466d8e3ec722"
   },
   "outputs": [],
   "source": [
    "# All Import Statements Defined Here\n",
    "# Note: Do not add to this list.\n",
    "# ----------------\n",
    "\n",
    "import sys\n",
    "assert sys.version_info[0]==3\n",
    "assert sys.version_info[1] >= 5\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy as sp\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4q8E45iMvto"
   },
   "source": [
    "## Word Vectors\n",
    "\n",
    "Word Vectors are often used as a fundamental component for downstream NLP tasks, e.g. question answering, text generation, translation, etc., so it is important to build some intuitions as to their strengths and weaknesses. Here, you will explore word vectors derived from *Word2Vec*. \n",
    "\n",
    "**Note on Terminology:** The terms \"word vectors\" and \"word embeddings\" are often used interchangeably. The term \"embedding\" refers to the fact that we are encoding aspects of a word's meaning in a lower dimensional space. As [Wikipedia](https://en.wikipedia.org/wiki/Word_embedding) states, \"*conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension*\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fO3hIO8IMvtw"
   },
   "source": [
    "## Word Vectors \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "We shall explore the embeddings produced by word2vec. Please revisit the class notes and lecture slides for more details on the word2vec algorithm. Paper 1 review due May 4th, involves reading the  [word2vec  paper](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf),  reading it now might help you with this assignment.\n",
    "\n",
    "Run the following cells to load the word2vec vectors into memory. **Note**: If this is your first time to run these cells, i.e. download the embedding model, it will take a couple minutes to run. If you've run these cells before, rerunning them will load the model without redownloading it, which will take about 1 to 2 minutes. In *Colab*, the embeddings are downloaded to the server everytime you restart the notebook). For this reason, you may prefer to work on your local machine where the download only happens once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "bFrL7EjHMvtw"
   },
   "outputs": [],
   "source": [
    "def load_embedding_model():\n",
    "    \"\"\" Load Word2Vec Vectors\n",
    "        Return:\n",
    "            wv_from_bin: All the embeddings\n",
    "    \"\"\"\n",
    "    import gensim.downloader as api\n",
    "    wv_from_bin = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "    #print(\"Loaded vocab size %i\" % len(wv_from_bin.vocab.keys()))\n",
    "    return wv_from_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 797891,
     "status": "ok",
     "timestamp": 1649550154639,
     "user": {
      "displayName": "Ndapandula Nakashole",
      "userId": "12501637879061685337"
     },
     "user_tz": 420
    },
    "id": "aSWjn1UvMvtw",
    "outputId": "6169993d-2930-4fa4-931d-85d07008d4a4"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Run Cell to Load Word Vectors\n",
    "# Note: This will take a couple minutes\n",
    "# -----------------------------------\n",
    "wv_from_bin = load_embedding_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnhwd5uuMvtw"
   },
   "source": [
    "#### Note: If you are receiving a \"reset by peer\" error, rerun the cell to restart the download. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1sLJ47CMvtw"
   },
   "source": [
    "\n",
    "### Plot function\n",
    "Let's define a plot function that reduces the vectors from 300-dimensions to 2-dimensions, and visualises them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "gI86TKrUMvtw"
   },
   "outputs": [],
   "source": [
    "def display_pca_scatterplot(model, words=None, sample=0):\n",
    "    if words == None:\n",
    "        if sample > 0:\n",
    "            words = np.random.choice(list(model.key_to_index.keys()), sample)\n",
    "        else:\n",
    "            words = [ word for word in model.vocab ]\n",
    "        \n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "\n",
    "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQpvbQDFMvtx"
   },
   "source": [
    "### Question 1: Word2vec Plot Analysis [written] (2 points)\n",
    "\n",
    "Run the cell below to plot the 2D GloVe embeddings for `['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'iraq']`.\n",
    "\n",
    "What clusters together in 2-dimensional embedding space? What doesn't cluster together that you think should have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "executionInfo": {
     "elapsed": 572,
     "status": "ok",
     "timestamp": 1649550972955,
     "user": {
      "displayName": "Ndapandula Nakashole",
      "userId": "12501637879061685337"
     },
     "user_tz": 420
    },
    "id": "hV6LXSCEMvtx",
    "outputId": "340f76a3-70e8-41fb-ea77-6ac0a7ba388a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'iraq']\n",
    "display_pca_scatterplot(wv_from_bin, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0w7xtcUSMvtx"
   },
   "source": [
    "#### <font color=\"red\">Write your answer here.</font>\n",
    "I think country is one cluster such as Iraq, Kuwait. Unit for oil such as barrels and bqd is another cluster. Gas industry related words such as oil, petroleum and output are another cluster. General terms such as industry and energy cluster together. I think Ecuador should get closer to and cluster with Kuwait and Iraq as it's also a country name.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ny56x_fMvtx"
   },
   "source": [
    "### Cosine Similarity\n",
    "Now that we have word vectors, we need a way to quantify the similarity between individual words, according to these vectors. One such metric is cosine-similarity. We will be using this to find words that are \"close\" and \"far\" from one another.\n",
    "\n",
    "The [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) $s$ between two vectors $p$ and $q$ is defined as:\n",
    "\n",
    "$$s = \\frac{p \\cdot q}{||p|| ||q||}, \\textrm{ where } s \\in [-1, 1] $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEWzAas1Mvtx"
   },
   "source": [
    "### Question 2: Words with Multiple Meanings (2 points) [code + written] \n",
    "Polysemes and homonyms are words that have more than one meaning (see this [wiki page](https://en.wikipedia.org/wiki/Polysemy) to learn more about the difference between polysemes and homonyms ). Find a word with *at least two different meanings* such that the top-10 most similar words (according to cosine similarity) contain related words from *both* meanings. For example, \"leaves\" has both \"go_away\" and \"a_structure_of_a_plant\" meaning in the top 10, and \"rock\" has both \"music\" and \"stone\". You will probably need to try several polysemous or homonymic words before you find one. \n",
    "\n",
    "Please state the word you discover and the multiple meanings that occur in the top 10. Why do you think many of the polysemous or homonymic words you tried didn't work (i.e. the top-10 most similar words only contain **one** of the meanings of the words)?\n",
    "\n",
    "**Note**: You should use the `wv_from_bin.most_similar(word)` function to get the top 10 similar words. This function ranks all other words in the vocabulary with respect to their cosine similarity to the given word. For further assistance, please check the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.most_similar)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "4kTQ066WMvtx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('falling', 0.6371317505836487),\n",
       " ('falls', 0.6107184290885925),\n",
       " ('drop', 0.591251790523529),\n",
       " ('tumble', 0.5696445107460022),\n",
       " ('rise', 0.5596301555633545),\n",
       " ('plummet', 0.5581283569335938),\n",
       " ('fell', 0.5548586845397949),\n",
       " ('spring', 0.541506826877594),\n",
       " ('Fall', 0.5406967401504517),\n",
       " ('sag', 0.5160202383995056)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "    wv_from_bin.most_similar('fall')\n",
    "    # ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6D9rhZJc1psV"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVll_55fMvtx"
   },
   "source": [
    "#### <font color=\"red\">Write your answer here.</font>\n",
    "I found fall. fall can mean one of the seasons and that's why we see spring and Fall in the list. Fall can also mean action tumble and that's why we see fall, drop pulmmet in the list too<br>\n",
    "I think one meaning of a specific polysemous words doesn't appear as frequent as the other one thus in the training data there aren't enough data for that meaning.Therefore, the most_similar words would only contain related words for the most frequent meaning for the polysemous word. <br>\n",
    "I think it could also be that among the top 10 similar words, the top ones are variants of the original words that all mean the same, which takes up the spot but didn't contribute to the polysemous. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2KRcfnVMvty"
   },
   "source": [
    "### Question 3: Analogies with Word Vectors [written] (2 points)\n",
    "Word vectors have been shown to *sometimes* exhibit the ability to solve analogies. \n",
    "\n",
    "As an example, for the analogy \"man : king :: woman : x\" (read: man is to king as woman is to x), what is x?\n",
    "\n",
    "In the cell below, we show you how to use word vectors to find x using the `most_similar` function from the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar)__. The function finds words that are most similar to the words in the `positive` list and most dissimilar from the words in the `negative` list (while omitting the input words, which are often the most similar; see [this paper](https://www.aclweb.org/anthology/N18-2039.pdf)). The answer to the analogy will have the highest cosine similarity (largest returned numerical value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "YysbnlIWMvty"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118193507194519),\n",
      " ('monarch', 0.6189674735069275),\n",
      " ('princess', 0.5902431011199951),\n",
      " ('crown_prince', 0.5499460697174072),\n",
      " ('prince', 0.5377321243286133),\n",
      " ('kings', 0.5236844420433044),\n",
      " ('Queen_Consort', 0.5235945582389832),\n",
      " ('queens', 0.518113374710083),\n",
      " ('sultan', 0.5098593831062317),\n",
      " ('monarchy', 0.5087411999702454)]\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to answer the analogy -- man : king :: woman : x\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'king'], negative=['man']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOezwAj0Mvty"
   },
   "source": [
    "Let $m$, $k$, $w$, and $x$ denote the word vectors for `man`, `king`, `woman`, and the answer, respectively. Using **only** vectors $m$, $k$, $w$, and the vector arithmetic operators $+$ and $-$ in your answer, what is the expression in which we are maximizing cosine similarity with $x$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeJ07PABMvty"
   },
   "source": [
    "#### <font color=\"red\">Write your answer here.</font>\n",
    "k-m+w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaHnrgCeMvty"
   },
   "source": [
    "### Question 4: Finding Analogies [code + written]  (1 point)\n",
    "Find an example of analogy that holds according to these vectors (i.e. the intended word is ranked top). In your solution please state the full analogy in the form x:y :: a:b. If you believe the analogy you came up might not be obvious to the TAs, explain why the analogy holds in one or two sentences.\n",
    "\n",
    "**Note**: You may have to try many analogies to find one that works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "EpcYBn5yMvty"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Beijing', 0.8216201663017273),\n",
      " ('Shanghai', 0.7951419949531555),\n",
      " ('Guangzhou', 0.6529654860496521),\n",
      " ('Beijng', 0.6465169787406921),\n",
      " ('Chinese', 0.6439486742019653),\n",
      " ('Shenzhen', 0.6439114809036255),\n",
      " ('Hong_Kong', 0.6337778568267822),\n",
      " ('Taipei', 0.6317877769470215),\n",
      " ('Chongqing', 0.6239100098609924),\n",
      " ('Hangzhou', 0.6204276084899902)]\n"
     ]
    }
   ],
   "source": [
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=['China', 'Tokyo'], negative=['Japan']))\n",
    "    # ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fhwe3RO-Mvty"
   },
   "source": [
    "#### <font color=\"red\">Write your answer here.</font>\n",
    "Tokyo:Japan :: Beijing: China. I tried to want the capital of China which is Beijing, in analogy to Japan and Tokyo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "se6pOUqQMvtz"
   },
   "source": [
    "### Question 5: Incorrect Analogy [code + written] (2 point)\n",
    "Find an example of analogy that does *not* hold according to these vectors. In your solution, state the intended analogy in the form x:y :: a:b, and state the (incorrect) value of b according to the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "nuZg40J8Mvtz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('puppy', 0.49447813630104065),\n",
      " ('dogs', 0.47720712423324585),\n",
      " ('pup', 0.45177432894706726),\n",
      " ('golden_retriever', 0.44033271074295044),\n",
      " ('Dog', 0.43949854373931885),\n",
      " ('black_Labrador_retriever', 0.4351273477077484),\n",
      " ('German_shepherd', 0.42699480056762695),\n",
      " ('barks', 0.4132434129714966),\n",
      " ('pooch', 0.408460795879364),\n",
      " ('puppyhood', 0.40548965334892273)]\n"
     ]
    }
   ],
   "source": [
    "    # ------------------\n",
    "    # Write your implementation here\n",
    "    pprint.pprint(wv_from_bin.most_similar(positive=['dog', 'croaks'], negative=['toad']))\n",
    "\n",
    "    # ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvmrmj2oMvtz"
   },
   "source": [
    "#### <font color=\"red\">Write your answer here.</font>\n",
    "The intended pair is toad:croaks :: dog barks<br>\n",
    "B is bark.<br>\n",
    "I'm using word vectors to find the behavior word specific to the animal. For example if toad is croaks, dog can be barks or wags. However the top words are not even verbs but similar nouns to dogs. Thus this is am example of incorrect analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDyys-8DMvtz"
   },
   "source": [
    "### Question 6: Guided Analysis of Bias in Word Vectors [written] (2 point)\n",
    "\n",
    "It's important to be cognizant of the biases (gender, race, sexual orientation etc.) implicit in our word embeddings. Bias can be dangerous because it can reinforce stereotypes through applications that employ these models.\n",
    "\n",
    "Run the cell below, to examine (a) which terms are most similar to \"woman\" and \"worker\" and most dissimilar to \"man\", and (b) which terms are most similar to \"man\" and \"worker\" and most dissimilar to \"woman\". Point out the difference between the list of female-associated words and the list of male-associated words, and explain how it is reflecting gender bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "mTyRfo1DMvtz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('workers', 0.6582455635070801),\n",
      " ('employee', 0.5805293917655945),\n",
      " ('nurse', 0.5249922275543213),\n",
      " ('receptionist', 0.5142489671707153),\n",
      " ('migrant_worker', 0.5001609921455383),\n",
      " ('Worker', 0.4979270398616791),\n",
      " ('housewife', 0.48609834909439087),\n",
      " ('registered_nurse', 0.4846191108226776),\n",
      " ('laborer', 0.48437267541885376),\n",
      " ('coworker', 0.48212409019470215)]\n",
      "\n",
      "[('workers', 0.5590360164642334),\n",
      " ('laborer', 0.54481041431427),\n",
      " ('foreman', 0.5192232131958008),\n",
      " ('Worker', 0.5161596536636353),\n",
      " ('employee', 0.5094279646873474),\n",
      " ('electrician', 0.49481216073036194),\n",
      " ('janitor', 0.48718899488449097),\n",
      " ('bricklayer', 0.4825313091278076),\n",
      " ('carpenter', 0.47498995065689087),\n",
      " ('workman', 0.4642517864704132)]\n"
     ]
    }
   ],
   "source": [
    "# Run this cell\n",
    "# Here `positive` indicates the list of words to be similar to and `negative` indicates the list of words to be\n",
    "# most dissimilar from.\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'worker'], negative=['man']))\n",
    "print()\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=['man', 'worker'], negative=['woman']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1t37qzjMvtz"
   },
   "source": [
    "#### <font color=\"red\">Write your answer here.</font>\n",
    "When we look for woman-occupations, the list indicates(registered)nurse, housewife, receptionist, migrant_worker and coworker are most similar to woman while for man-occupations, foreman, electrician, janitor,  bricklayer, carpenter and workman are most similar to man. These conforms to social stereotype and expection for different genders. For example, women are associated with jobs indoor and requiring less physical power while men are with jobs requuire more physical power and outdoor. I think this difference reflects gender bias because it refelcts the social expectation and stereotype for different genders. I think when training the corpus, jobs such as nurse and receptionist coappear more with female in the corpus because women are expected to fulfill such indoor jobs while men are expected to do jobs with leadership such as foremand and with physical strength like bricklayer. Thus these two lists reflect the gender bias in workplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PI5PRZVYMvtz"
   },
   "source": [
    "### Question 7: Independent Analysis of Bias in Word Vectors [code + written]  ( 1.5 point)\n",
    "\n",
    "Use the `most_similar` function to find another case where some bias is exhibited by the vectors. Please briefly explain the example of bias that you discover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "fW5_tJNeMvtz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('workers', 0.566972553730011),\n",
      " ('laborer', 0.4886777102947235),\n",
      " ('employee', 0.47609761357307434),\n",
      " ('Worker', 0.4466555714607239),\n",
      " ('migrant_worker', 0.4337000846862793),\n",
      " ('electrician', 0.43322888016700745),\n",
      " ('manual_laborer', 0.41811051964759827),\n",
      " ('mineworker', 0.41758155822753906),\n",
      " ('ironworker', 0.4103507697582245),\n",
      " ('carpenter', 0.4076002836227417)]\n",
      "\n",
      "[('workers', 0.5764017105102539),\n",
      " ('employee', 0.5437594652175903),\n",
      " ('Worker', 0.4991253614425659),\n",
      " ('forklift_operator', 0.47370877861976624),\n",
      " ('laborer', 0.47304514050483704),\n",
      " ('supervisor', 0.4569508135318756),\n",
      " ('coworker', 0.44874605536460876),\n",
      " ('crane_operator', 0.4459839165210724),\n",
      " ('migrant_worker', 0.43697965145111084),\n",
      " ('superviser', 0.42401862144470215)]\n"
     ]
    }
   ],
   "source": [
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=['Black', 'worker'], negative=['White']))\n",
    "print()\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=['White', 'worker'], negative=['Black']))\n",
    "\n",
    "\n",
    "    # ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArSm9i1rMvt0"
   },
   "source": [
    "#### <font color=\"red\">Write your answer here.</font>\n",
    "I tried to explore race bias in workplace. For Black worker, the list returns manual_laborer, mineworker, ironworker and carpentet, while for White worker, the list returns forklife_operator, supervisor, coworker, crane_operator and superviser. The difference in the output refelcts the race bias where workers of black race are associated with manual labor that has minimum pay but requires more labor while workers of white race are assocaited with skilled jobs such as forklift and crane operator that have higher pay and require less physical power. Also, only superviser is associated with white worker, which can further demonstrates the race bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bL9C0xPyMvt0"
   },
   "source": [
    "### Question 8: Thinking About Bias [written] (Bonus: 2 points)\n",
    "\n",
    "Give one explanation of how bias gets into the word vectors. What is an experiment that you could do to test for or to measure this source of bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSbxt9EYMvt0"
   },
   "source": [
    "#### <font color=\"red\">Write your answer here.</font>\n",
    "One explanation of how bias get into the word vectors would be the input of the data that the model is trained on. The contexts of a gender/race neutral word acquire stereotype and impose on that word and then incoporate the bias in the word vector. For example, word nurse coappear more frequent with feamle context than male context.To measure the source of bias, we can plot the word vector in space to see if there is any stereotypical clustering and paring similar to the appropriate clustering and pairng. For example, if we see doctor and nurse correlate the same way as men and women, we know there is gender bias in the word-embedding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nx4fdrswMvt0"
   },
   "source": [
    "# <font color=\"blue\"> Submission Instructions</font>\n",
    "\n",
    "1. Click the Save button at the top of the Jupyter Notebook.\n",
    "2. Select Cell -> All Output -> Clear. This will clear all the outputs from all cells (but will keep the content of all cells). \n",
    "2. Select Cell -> Run All. This will run all the cells in order, and will take several minutes.\n",
    "3. Once you've rerun everything, select File -> Download as -> PDF via LaTeX (If you have trouble using \"PDF via LaTex\", you can also save the webpage as pdf. <font color='blue'> Make sure all your solutions especially the coding parts are displayed in the pdf</font>, it's okay if the provided codes get cut off because lines are not wrapped in code cells).\n",
    "4. Look at the PDF file and make sure all your solutions are there, displayed correctly. The PDF is the only thing your graders will see!\n",
    "5. Submit your PDF on Gradescope.\n",
    "\n",
    "\n",
    "#### <font color=\"blue\"> Acknowledgements</font>\n",
    "This assignment is based on an assignment developed by Chris Manning"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [
    "cnhwd5uuMvtw",
    "0w7xtcUSMvtx",
    "9Ny56x_fMvtx",
    "XVll_55fMvtx",
    "xeJ07PABMvty",
    "NaHnrgCeMvty",
    "Fhwe3RO-Mvty",
    "se6pOUqQMvtz",
    "jvmrmj2oMvtz",
    "HDyys-8DMvtz",
    "N1t37qzjMvtz",
    "ArSm9i1rMvt0",
    "bL9C0xPyMvt0",
    "CSbxt9EYMvt0"
   ],
   "name": "CSE156_Assignment2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
